---
* # Apach SPark

  * ### 맵리듀스(MapReduce)
    * 슈퍼컴퓨터 없이 서버를 여러대 연결해 빅데이터 분석을 가능하게함
    * 사용중에 여러가지 문제점 등장 -> **Apach Spark의 등장** (맵리듀스와 비슷한 목적의 업무를 수행하는데, 메모리를 활용한 굉장히 빠른 데이터처리가 특징)
  * ### Hadoop
    * 대용량 데이터를 분산처리 할 수 있는 자바기반 오픈소스 프레임워크
    * 분산 저장 기술인 HDFS와 분산처리 기술인 MapReduce가 굉장한 장점이며, 오픈소스가 되면서 굉장한 주목과 인기를 끌고있음
  * ### 분산프로그래밍
    * 간단히 말해 여러 디스크로부터 데이터를 한번에 읽은것을 말함.
    * 하드디스크 드라이브 저장용량이 대폭 증가한 반면, 엑세스 속도는 그에미치지 못해 드라이브를 읽는데 오랜 시간이 걸림
    * 이 시간을 줄이기위해 하둡을 사용
    * 예를들어 100개의 드라이브가 있고 각 드라이브에 1/100만큼 데이터를 저장했다고 가정했을 때 이것이 병렬로 동작한다면 2분 내에 데이터 Read가 가능해짐
  * ### 하둡의 분산처리 단계
    * 하둡에서 분산처리를 위해 사용하는 기술인 **맵리듀스(MapReduce)** 는 단계별로 데이터를 처리
    * 클러스터에서 읽기 &#9654; 동작 실행 &#9654; 클러스터에 결과 기록 &#9654; 데이터 업데이트된 내용 읽기 &#9654; 다음 동작 실행 &#9654; 클러스터에 결과 기록 이런식의 5단계에 걸친 데이터 처리를 진행
  * ### 하둡(Haddop)을 **사용하는 이유**
    * 기존 관계형 데이터베이스 시스템(RDBMS)은 비싼 비용이 듬.
    * 하지만 하둡은 오픈소스로서 비용이 거의 들지 않음
    * 특히 빅데이터를 감당하려면 비례되는 비용이 들기마련, but 하둡은 분산 컴퓨팅 방식으로 구축 비용이 저렴하며, 비용 대비 데이터처리가 빠름
    * 더불어 장애를 대비하여 매번 운영한 이후의 결과들을 디스크에 기록하기때문에 문제가 발생할 때 기록된 결과를 활용해 문제를 파악하고 해결하기 쉽다는 장점이 있음
  * ### <span style="color:RED"> 아파치 스파크(Apache spar) </span>
    * 스파크는 빅데이터 Workload에 주로 사용되는 분산처리 시스템, 하둡과 마찬가지로 오픈소스
    * 빠른 성능을 위해 인 메모리 캐싱과 최적화된 실행을 사용하고 일반 배치처리, 스트리밍 분석, 머신러닝, 그래프 데이터베이스 및 임시 쿼리를 지원하는것이 특징
    * 하둡 없이 클라우드 기반 데이터 플랫폼과 융합하며, 원래는 하둡을 이용한 정보활용을 위한 데이터 프로세싱 툴
    * 기존의 하둡을 통해 끌어오는 데이터들은 시간 소요가 크기 때문에 실시간 처리를 위한 업무에서 사용하기위해 개발
    * 스파크는 맵리듀스와 유사한 일괄 처리기능, 실시간 데이터 처리 기능, SQL과 유사한 정형 데이터 처리 기능, 그래프 알고리즘, 머신러닝 알고리즘을 모두 단일 프레임워크와 통합. 마치 빅데이터 애플리케이션에 필요한 대부분의 요구사항을 만족시킬 수 있는 원스톱 편의점
    * 일부 애플리케이션은 스파크를 사용하기에 적합하지는 않음. 스파크는 분산 아키텍처이기 때문에 처리 시간에 약간의 overhead가 필연적으로 발생. 대량의 데이터를 다룰 때는 오버헤드가 무시 될 수 있는 수준이나, 단일 머신에서도 충분이 처리 가능한 데이터셋을 다룰때는 작은 데이터셋 연산에 최적화덴 프레임워크 사용하는것이 효율적
    * 스파크는 온라인 트랜잭션 처리(Online Transaction Processing, OLTP) 애플리케이션을 염두하고 설계되지 않아 적합하지 않음.
    * 즉 대량의 원자성(Atomicity)트랜잭션을 빠르게 처리해야 하는 작업에는 스파크가 적합하지 않음.
    * 반면 일괄 처리작업이나 데이터 마이닝 같은 **온라인 분석처리(Online Analytical Processing, OLAP) 작업에서는 적합**
  * ### 아파치 스파크의 분산처리 단계
    * 스파크는 전체의 데이터셋을 한번에 처리함
    
    * ==클러스터에서 데이터 읽기 &#9654; 애널리틱스 운영 수행 및 결과값 클러스터 입력 동작과 같은 전 과정이 동시에 진행==
    * 하둡과 달리 2단계 처리
    * 일반적 상황에서 스파크의 데이터 처리속도는 **하둡에 비해 월등히 빠름**
    * 배치 프로세싱의 경우 하둡의 MapReduce에 비하여 Spark가 10배 더 빠르고, 인메모리 애널리스틱의 경우에는 100배 더 빠른 수행속도를 냄.

    ### Result
    >* **하둡**
      데이터를 운영하고, 리포팅 요구들이 대체로 정적이거나 배치 모드 프로세싱을 기다릴 수 있다면 MapReduce 만으로도 문제 없이 처리 가능
    >* **Spark**
    스트리밍 데이터 처리와 머신러닝 알고리즘 처럼 애플리케이션과의 복합적 운영이 필요할 때 적합(ex> 실시간 마케팅 캠페인, 온라인 상품 추천, 사이버 보안 분석 등)
    >* **하둡**은 HDFS(Hadoop Distributed File System)이라는 분산형 파일 시스템과 프로세싱 컴포넌트인 맵리듀스를 제공. 때문에 스파크가 필수적이지 않음. 마찬가지로 **스파크**도 하둡의 HDFS 외에도 다른 클라우드 기반 데이터 플랫폼과도 융합될 수 있어 **하둡이 반드시 필요한것은 아님**
    >* 다만 스파크 개발 당시에는 원래 하둡용으로 설계된 솔루션이기 때문에 하둡과 스파크를 함께 사용할 때 최상의 효과를 낼 수 있음
